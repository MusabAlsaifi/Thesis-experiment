This project focuses on fine-tuning and evaluating language models for mental health counseling conversations. The goal is to develop a model that can provide empathetic, personalized, and therapeutic responses while maintaining a professional and non-judgmental tone.

## Data

The project utilizes a combination of datasets for fine-tuning and evaluation:

- **Therapy Session Transcripts:** Transcripts from therapy sessions collected from books and podcasts.
- **Mental Health Counseling Conversations Dataset:** A dataset from Hugging Face containing question-answer pairs related to mental health.
- **Synthetic Data:** Data generated by GPT-4 to augment the training data.

## Fine-tuning

The project fine-tunes a GPT-4-mini model using the OpenAI API. Hyperparameters such as the number of epochs, learning rate multiplier, and batch size are specified. 

## Evaluation

The project leverages LangSmith for evaluation, implementing LLM-as-a-judge evaluation with GPT-4o and GPT-4o-mini. Evaluation criteria are defined based on factors like empathy, emotional validation, and client-centered approach. Separate evaluation prompts are created for individual and pairwise comparisons.

## Prompts and Models

The project experiments with different prompts, including:

- **Standard Prompt**
- **Step-by-step Prompt**
- **Chain-of-Thought (CoT) Prompt**
- **Few-Shot-CoT Prompt**

The following models are evaluated:

- Fine-tuned GPT-4-mini
- GPT-4-mini
- LLaMa3-8B
- Mixtral-7B
- Gemma2-9B
